"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[4546],{9293:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>o,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>c,toc:()=>h});var a=n(5893),i=n(1151);const s={sidebar_label:"chat",title:"agentchat.chat"},r=void 0,c={id:"reference/agentchat/chat",title:"agentchat.chat",description:"ChatResult Objects",source:"@site/docs/reference/agentchat/chat.md",sourceDirName:"reference/agentchat",slug:"/reference/agentchat/chat",permalink:"/autogen/docs/reference/agentchat/chat",draft:!1,unlisted:!1,editUrl:"https://github.com/microsoft/autogen/edit/main/website/docs/reference/agentchat/chat.md",tags:[],version:"current",frontMatter:{sidebar_label:"chat",title:"agentchat.chat"},sidebar:"referenceSideBar",previous:{title:"assistant_agent",permalink:"/autogen/docs/reference/agentchat/assistant_agent"},next:{title:"conversable_agent",permalink:"/autogen/docs/reference/agentchat/conversable_agent"}},o={},h=[{value:"ChatResult Objects",id:"chatresult-objects",level:2},{value:"chat_history",id:"chat_history",level:4},{value:"summary",id:"summary",level:4},{value:"cost",id:"cost",level:4},{value:"human_input",id:"human_input",level:4},{value:"initiate_chats",id:"initiate_chats",level:4}];function l(e){const t={code:"code",h2:"h2",h4:"h4",p:"p",pre:"pre",...(0,i.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.h2,{id:"chatresult-objects",children:"ChatResult Objects"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"@dataclass\nclass ChatResult()\n"})}),"\n",(0,a.jsx)(t.p,{children:"(Experimental) The result of a chat. Almost certain to be changed."}),"\n",(0,a.jsx)(t.h4,{id:"chat_history",children:"chat_history"}),"\n",(0,a.jsx)(t.p,{children:"The chat history."}),"\n",(0,a.jsx)(t.h4,{id:"summary",children:"summary"}),"\n",(0,a.jsx)(t.p,{children:"A summary obtained from the chat."}),"\n",(0,a.jsx)(t.h4,{id:"cost",children:"cost"}),"\n",(0,a.jsx)(t.p,{children:"The cost of the chat. a tuple of (total_cost, total_actual_cost), where total_cost is a dictionary of cost information, and total_actual_cost is a dictionary of information on the actual incurred cost with cache."}),"\n",(0,a.jsx)(t.h4,{id:"human_input",children:"human_input"}),"\n",(0,a.jsx)(t.p,{children:"A list of human input solicited during the chat."}),"\n",(0,a.jsx)(t.h4,{id:"initiate_chats",children:"initiate_chats"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",children:"def initiate_chats(chat_queue: List[Dict[str, Any]]) -> List[ChatResult]\n"})}),"\n",(0,a.jsx)(t.p,{children:"Initiate a list of chats."}),"\n",(0,a.jsxs)(t.p,{children:["args:\nchat_queue (List[Dict]): a list of dictionaries containing the information of the chats.\nEach dictionary should contain the input arguments for ",(0,a.jsx)(t.code,{children:"ConversableAgent.initiate_chat"}),'.\nMore specifically, each dictionary could include the following fields:\nrecipient: the recipient agent.\n- "sender": the sender agent.\n- "recipient": the recipient agent.\n- clear_history (bool): whether to clear the chat history with the agent. Default is True.\n- silent (bool or None): (Experimental) whether to print the messages for this conversation. Default is False.\n- cache (Cache or None): the cache client to be used for this conversation. Default is None.\n- max_turns (int or None): the maximum number of turns for the chat. If None, the chat will continue until a termination condition is met. Default is None.\n- "message" needs to be provided if the ',(0,a.jsx)(t.code,{children:"generate_init_message"}),' method is not overridden.\nOtherwise, input() will be called to get the initial message.\n- "summary_method": a string or callable specifying the method to get a summary from the chat. Default is DEFAULT_summary_method, i.e., "last_msg".\n- Supported string are "last_msg" and "reflection_with_llm":\nwhen set "last_msg", it returns the last message of the dialog as the summary.\nwhen set "reflection_with_llm", it returns a summary extracted using an llm client.\n',(0,a.jsx)(t.code,{children:"llm_config"}),' must be set in either the recipient or sender.\n"reflection_with_llm" requires the llm_config to be set in either the sender or the recipient.\n- A callable summary_method should take the recipient and sender agent in a chat as input and return a string of summary. E.g,\n',(0,a.jsx)(t.code,{children:'python                 def my_summary_method(                     sender: ConversableAgent,                     recipient: ConversableAgent,                 ):                     return recipient.last_message(sender)["content"]                 '}),'\n"summary_prompt" can be used to specify the prompt used to extract a summary when summary_method is "reflection_with_llm".\nDefault is None and the following default prompt will be used when "summary_method" is set to "reflection_with_llm":\n"Identify and extract the final solution to the originally asked question based on the conversation."\n"carryover" can be used to specify the carryover information to be passed to this chat.\nIf provided, we will combine this carryover with the "message" content when generating the initial chat\nmessage in ',(0,a.jsx)(t.code,{children:"generate_init_message"}),"."]}),"\n",(0,a.jsx)(t.p,{children:"returns:\n(list): a list of ChatResult objects corresponding to the finished chats in the chat_queue."})]})}function d(e={}){const{wrapper:t}={...(0,i.a)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(l,{...e})}):l(e)}},1151:(e,t,n)=>{n.d(t,{Z:()=>c,a:()=>r});var a=n(7294);const i={},s=a.createContext(i);function r(e){const t=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function c(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(s.Provider,{value:t},e.children)}}}]);